## Data Engineering mit Apache Spark auf AWS ##

# Aufgabenstellung:

Ziel dieses Projekts war die Entwicklung skalierbarer und robuster Datenpipelines zur Verarbeitung großer Datenmengen mithilfe von Apache Spark auf Amazon Web Services (AWS). Dabei wurden Spark-Komponenten wie RDDs, DataFrames, Spark SQL sowie Performanceoptimierungen angewendet. Die Datenverarbeitung erfolgte mit PySpark in verteilten Umgebungen unter Nutzung von Services wie Amazon EMR, AWS Glue und Redshift. Zudem wurden Integrationen mit Amazon Athena umgesetzt. Das Projekt umfasste sowohl die Entwicklung produktionsnaher ETL-Prozesse als auch die Visualisierung und Analyse großer Datenbestände – mit Fokus auf Effizienz, Skalierbarkeit und Wartbarkeit im Rahmen moderner Data-Engineering-Praktiken.
